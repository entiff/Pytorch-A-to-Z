{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Net(합성곱 신경망, CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CNN](https://github.com/shwksl101/Pytorch-A-to-Z/blob/master/img/cnn.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:15.288048Z",
     "start_time": "2018-10-28T15:25:15.265319Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Sequential(\n",
      "    (0): Linear(in_features=400, out_features=120, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc2): Sequential(\n",
      "    (0): Linear(in_features=120, out_features=84, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (fc3): Sequential(\n",
      "    (0): Linear(in_features=84, out_features=10, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Net(nn.Module): \n",
    "    def __init__ (self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size(filter), stride=1, padding=0, dilation=1, groups=1, bias=True)\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        self.conv1 = nn.Sequential(\n",
    "                            nn.Conv2d(1,6,5),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "        \n",
    "        # out_channels = 6이므로, in_channels = 6입니다.\n",
    "        # out_channels = 16, kernel_size = 5로 하겠습니다.\n",
    "        # kernel_size는 convolving하는 kernel의 크기를 뜻합니다. \n",
    "        # kernel = filter입니다.\n",
    "        self.conv2 = nn.Sequential(\n",
    "                            nn.Conv2d(6,16,5),\n",
    "                            nn.ReLU()\n",
    "                            )\n",
    "        \n",
    "        # torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)\n",
    "        \n",
    "        # an affine operation: y = Wx +b (어파인 연산)\n",
    "        # typical cnn에서는 conv layer 뒤에 fully connected layer를 붙입니다.\n",
    "        self.fc1 = nn.Sequential(\n",
    "                        nn.Linear(16*5*5,120),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "                        nn.Linear(120,84),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "                        nn.Linear(84,10),\n",
    "                        nn.ReLU()\n",
    "                        )\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        # -1은 몇개가 들어올 지 모른다는 뜻입니다.\n",
    "        # num_flat_features는 1x? 형태로 flat하게 reshape합니다.\n",
    "        \n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # x.size()[0]은 input 갯수이기 때문에 제외합니다. 각각의 input값들을 1x?형태로 reshape합니다.\n",
    "    # torch.randn(5,3,3) 을 생각해보면 개수를 의미하는 5를 제외하고 3x3 = 9의 값을 return합니다.\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- forward 이후에 backward는 autograd를 이용해 자동으로 정의할 수 있습니다.\n",
    "- net의 weight들은 net.parameters()에 의해 return됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:16.712187Z",
     "start_time": "2018-10-28T15:25:16.707199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())\n",
    "# conv1의 weight 벡터 사이즈입니다. 5x5 행렬 6개겠죠? print(params)로 확인해보세요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:17.164921Z",
     "start_time": "2018-10-28T15:25:17.158481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0027, 0.0000, 0.0838, 0.0180, 0.0000, 0.0000, 0.0000, 0.0000, 0.0576,\n",
      "         0.0894]], grad_fn=<ThresholdBackward0>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1,1,32,32) #32x32 하나를 input으로 넣어줍니다. 차원에 유의하세요.\n",
    "\n",
    "out = net(input) \n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input이 왜 1x1x32x32로 들어갈까요? 이유는 간단합니다. torch.nn에서는 mini-batch만 지원합니다.  \n",
    "따라서 nnconv2d는 nsamples x nChannels x Height x Width의 4차원 tensor를 입력으로 합니다.  \n",
    "1개의 값을 input channel(1)에 넣으니 1x1이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:19.226038Z",
     "start_time": "2018-10-28T15:25:19.219590Z"
    }
   },
   "outputs": [],
   "source": [
    "# 각 연산의 gradient가 저장되어 있는데 그 값을 초기화 해줍니다.\n",
    "\n",
    "net.zero_grad()\n",
    "out.backward(torch.randn(1,10)) # loss를 정의하지 않았으므로 임의의 값으로 back prop을 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "torch.tensor는 autograd 연산을 지원합니다. 또한 tensor의 gradient를 갖고 있습니다. back prop할때마다 이를 초기화 해야 합니다.  \n",
    "nn.module은 weight를 캡슐화해서 GPU로의 이동, 내보내기, 불러오기 등의 작업을 도와줍니다.  \n",
    "nn.parameter는 tensor의 종류로 module에 할당될 때 자동으로 weight로 등록됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss function(손실 함수)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "손실함수는 output, target을 한 쌍의 입력으로 받아 ouput이 target으로부터 얼마나 떨어져 있는지를 추정하는 값을 계산합니다.  \n",
    "output은 net이 계산한 추정값(출력), target은 실제 값(정답)입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:21.414092Z",
     "start_time": "2018-10-28T15:25:21.405167Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.1541, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.arange(1,11, dtype = torch.float)\n",
    "# torch.unsqueeze(target, dim=0)과 같습니다. reshape가 목적입니다.\n",
    "target = target.view(1,-1)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output,target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-17T12:01:01.630285Z",
     "start_time": "2018-10-17T12:01:01.623839Z"
    }
   },
   "source": [
    "loss.grad_fn는 다음과 같은 연산 그래프를 따릅니다.  \n",
    "\n",
    "input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d  \n",
    "-> view -> linear -> relu -> linear -> relu -> linear  \n",
    "-> MSELoss  \n",
    "-> loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:23.217006Z",
     "start_time": "2018-10-28T15:25:23.210591Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000024EAE19B0F0>\n",
      "<ThresholdBackward0 object at 0x0000024EAE19B278>\n"
     ]
    }
   ],
   "source": [
    "#.grad_fn.next_functions[]를 통해 gradient function을 추적할 수 있습니다.\n",
    "print(loss.grad_fn) # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back Propagation(역전파)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:25:24.738363Z",
     "start_time": "2018-10-28T15:25:24.722986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([-0.0061,  0.0356, -0.0306,  0.0491, -0.0256, -0.0011])\n",
      "tensor([ 0.0403,  0.0246,  0.0176, -0.0658,  0.0351,  0.0493,  0.0244,  0.0661,\n",
      "         0.0000,  0.0183, -0.0116, -0.0835,  0.0199,  0.0397, -0.0953, -0.0125])\n",
      "tensor([ 0.0000,  0.0847,  0.0065,  0.0721, -0.0674,  0.0000,  0.0000, -0.0628,\n",
      "         0.0000,  0.0000, -0.0421,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -0.0843,  0.0000,  0.0032, -0.0516,  0.0000,  0.0000, -0.0905,  0.0000,\n",
      "         0.0000,  0.0000, -0.0900, -0.0348,  0.0000, -0.0361, -0.0816, -0.1879,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.0627,  0.0000,  0.0000, -0.0842,\n",
      "         0.0000,  0.0000,  0.0196,  0.0768,  0.0000, -0.0892, -0.0722,  0.0000,\n",
      "        -0.0691,  0.0000, -0.0428,  0.1016,  0.0000,  0.0000,  0.0626,  0.0143,\n",
      "         0.0000,  0.0556,  0.0000,  0.0000,  0.0000,  0.0440,  0.0000, -0.0156,\n",
      "         0.0587,  0.0000,  0.0441,  0.0000,  0.0000, -0.0401,  0.0000,  0.0000,\n",
      "         0.0000, -0.0869,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000, -0.0339,\n",
      "         0.0000,  0.0293,  0.0677, -0.0391,  0.0000,  0.0891,  0.0896,  0.0000,\n",
      "         0.0325,  0.0000,  0.0000,  0.0000, -0.0955,  0.0637,  0.0000,  0.0199,\n",
      "         0.0000,  0.0000,  0.0670, -0.1298,  0.0000, -0.0091,  0.1320,  0.0000,\n",
      "         0.0000,  0.0000,  0.0613,  0.0084,  0.0000,  0.0054,  0.0000,  0.0329,\n",
      "         0.0000, -0.0060,  0.0000, -0.0495, -0.0127, -0.0595, -0.0356,  0.0000])\n",
      "tensor([ 0.0000,  0.1887,  0.0000,  0.0000,  0.0000,  0.0000,  0.0257,  0.4058,\n",
      "        -0.1969, -0.0726, -0.2305,  0.0000, -0.0286,  0.0000, -0.0575, -0.2727,\n",
      "         0.0000, -0.1594, -0.1181,  0.0000,  0.0000, -0.1722,  0.0000,  0.0000,\n",
      "        -0.0347, -0.4261,  0.0000,  0.0000,  0.1782,  0.2358,  0.0000,  0.0000,\n",
      "        -0.0198, -0.0597,  0.3108, -0.0968, -0.1484,  0.0000, -0.2446,  0.0000,\n",
      "         0.2368, -0.0605,  0.0000, -0.0151, -0.0422, -0.1554,  0.2196,  0.0000,\n",
      "         0.0243,  0.0000, -0.2996, -0.2675,  0.0000, -0.0369,  0.0000, -0.0496,\n",
      "         0.0000,  0.0000,  0.1770, -0.2450, -0.1316,  0.0000, -0.2006, -0.1479,\n",
      "         0.0000,  0.0000,  0.0000,  0.0000,  0.2586,  0.0000,  0.0000, -0.2421,\n",
      "         0.3097,  0.0000,  0.0123,  0.0000,  0.2526,  0.1569,  0.1553,  0.0000,\n",
      "         0.1585,  0.0516, -0.0629,  0.0000])\n",
      "tensor([-0.1995,  0.0000, -0.5832, -0.7964,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "        -1.7885, -1.9821])\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "\n",
    "print('conv1.bias.grad before backward')\n",
    "\n",
    "# nn.Sequential로 여러 function을 묶어뒀을 경우 []를 통해 순서대로 출력할 수 있습니다.\n",
    "print(net.conv1[0].bias.grad)\n",
    "\n",
    "# 보다시피 값이 모두 0으로 초기화되어 있습니다.\n",
    "\n",
    "loss.backward() # gradient를 계산합니다.\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1[0].bias.grad)\n",
    "print(net.conv2[0].bias.grad)\n",
    "print(net.fc1[0].bias.grad)\n",
    "print(net.fc2[0].bias.grad)\n",
    "print(net.fc3[0].bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight update(가중치 갱신)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-28T15:27:11.839907Z",
     "start_time": "2018-10-28T15:27:11.824529Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(net.parameters(),lr=0.01) #Adam optimizer를 사용합니다. learning rate는 0.01입니다.\n",
    "\n",
    "# 학습 과정(가중치 갱신 과정)은 다음과 같습니다.\n",
    "optimizer.zero_grad() # 기존의 변화도에 대해 누적되는 것을 막기 위해 zero_grad()로 초기화합니다.\n",
    "output = net(input)\n",
    "loss = criterion(output,target)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
