{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice - Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:17:05.932838Z",
     "start_time": "2018-10-25T02:17:05.633281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:17:10.894808Z",
     "start_time": "2018-10-25T02:17:10.887863Z"
    }
   },
   "outputs": [],
   "source": [
    "# tensor를 생성하고 function을 지정합니다.\n",
    "\n",
    "x = torch.tensor([-2.0],requires_grad=True)\n",
    "y = torch.tensor([5.0],requires_grad=True)\n",
    "z = torch.tensor([-4.0], requires_grad=True)\n",
    "\n",
    "q =  x+y\n",
    "f = q*z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:17:16.201693Z",
     "start_time": "2018-10-25T02:17:16.193755Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.], requires_grad=True)\n",
      "tensor([5.], requires_grad=True)\n",
      "tensor([-4.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:17:17.700990Z",
     "start_time": "2018-10-25T02:17:17.695472Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], grad_fn=<AddBackward0>)\n",
      "tensor([-12.], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(q)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:17:51.878707Z",
     "start_time": "2018-10-25T02:17:51.872176Z"
    }
   },
   "outputs": [],
   "source": [
    "#class로 하면 retain_graph=True 옵션을 안넣어도 됩니다. 차후에 다루겠습니다.\n",
    "\n",
    "f.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:18:10.806390Z",
     "start_time": "2018-10-25T02:18:10.799940Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x000001FC6D225470>\n",
      "<AddBackward0 object at 0x000001FC6D2251D0>\n"
     ]
    }
   ],
   "source": [
    "# .grad_fn을 통해 gradient function을 알 수 있습니다.\n",
    "\n",
    "print(f.grad_fn)\n",
    "print(q.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:18:11.613323Z",
     "start_time": "2018-10-25T02:18:11.602925Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-4.])\n",
      "tensor([-4.])\n",
      "tensor([3.])\n"
     ]
    }
   ],
   "source": [
    "# .grad로 gradient를 출력할 수 있습니다.\n",
    "\n",
    "print(x.grad)\n",
    "print(y.grad)\n",
    "print(z.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:18:20.224186Z",
     "start_time": "2018-10-25T02:18:20.215754Z"
    }
   },
   "source": [
    "## TODO - Backpropagation\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1Jvlk56B36HPyihMRACOxUFm7FwUTVL3v\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:20:31.011694Z",
     "start_time": "2018-10-25T02:20:31.002233Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.7311], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 위의 directed acyclic graph를 구현해보겠습니다.\n",
    "\n",
    "w0 = torch.tensor([2.0], requires_grad=True)\n",
    "x0 = torch.tensor([-1.0],  requires_grad=True)\n",
    "w1 = torch.tensor([-3.0],  requires_grad=True)\n",
    "x1 = torch.tensor([-2.0],  requires_grad=True)\n",
    "w2 = torch.tensor([-3.0],  requires_grad=True)\n",
    "\n",
    "q1 =  w0*x0\n",
    "q2 = w1*x1\n",
    "f1 = q1+q2\n",
    "s = f1+w2\n",
    "\n",
    "out = torch.sigmoid(s)\n",
    "print(out)\n",
    "out.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:21:47.223881Z",
     "start_time": "2018-10-25T02:21:47.210377Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1966])\n",
      "tensor([0.3932])\n",
      "tensor([-0.3932])\n",
      "tensor([-0.5898])\n",
      "tensor([0.1966])\n"
     ]
    }
   ],
   "source": [
    "print(w0.grad)\n",
    "print(x0.grad)\n",
    "print(w1.grad)\n",
    "print(x1.grad)\n",
    "print(w2.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice - Weight Update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-10-25T02:23:23.638406Z",
     "start_time": "2018-10-25T02:23:23.606549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================\n",
      "First Prediction\n",
      "tensor([0.7311], grad_fn=<SigmoidBackward>)\n",
      "=====================\n",
      "Original Weight\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([-3.], requires_grad=True)\n",
      "tensor([-3.], requires_grad=True)\n",
      "=====================\n",
      "Gradient\n",
      "tensor([0.1058])\n",
      "tensor([0.2115])\n",
      "tensor([-0.1058])\n",
      "=====================\n",
      "Updated Weight\n",
      "tensor([1.9894], requires_grad=True)\n",
      "tensor([-3.0212], requires_grad=True)\n",
      "tensor([-2.9894], requires_grad=True)\n",
      "=====================\n",
      "Second Prediction\n",
      "tensor([0.7433], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 위 graph의 weight update 과정을 자세히 보겠습니다.\n",
    "\n",
    "w0 = torch.tensor([2.0], requires_grad=True)\n",
    "x0 = torch.tensor([-1.0], requires_grad=True)\n",
    "w1 = torch.tensor([-3.0], requires_grad=True)\n",
    "x1 = torch.tensor([-2.0], requires_grad=True)\n",
    "w2 = torch.tensor([-3.0], requires_grad=True)\n",
    "\n",
    "q1 = w0*x0\n",
    "q2 = w1*x1\n",
    "\n",
    "r = q1 + q2\n",
    "s = r + w2\n",
    "\n",
    "# weight를 수정하지 않고 prediction을 해봅니다.\n",
    "out = torch.sigmoid(s)\n",
    "print(\"=====================\")\n",
    "print(\"First Prediction\")\n",
    "print(out)\n",
    "\n",
    "# original weight를 출력합니다.\n",
    "print(\"=====================\")\n",
    "print(\"Original Weight\")\n",
    "print(w0)\n",
    "print(w1)\n",
    "print(w2)\n",
    "\n",
    "# optimizer와 loss를 정의합니다.\n",
    "# .backward()를 통해 gradient를 계산합니다.\n",
    "target = torch.tensor([1.0])\n",
    "optimizer = optim.SGD([w0, w1, w2], lr=0.1)\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(out, target)\n",
    "loss.backward()\n",
    "\n",
    "# 계산된 gradient를 출력합니다.\n",
    "print(\"=====================\")\n",
    "print(\"Gradient\")\n",
    "\n",
    "print(w0.grad)\n",
    "print(w1.grad)\n",
    "print(w2.grad)\n",
    "\n",
    "# weight를 update합니다.\n",
    "optimizer.step()\n",
    "\n",
    "print(\"=====================\")\n",
    "print(\"Updated Weight\") \n",
    "\n",
    "# w_new = w_origin - (learning_rate * gradient)\n",
    "print(w0) # w0_new = w0(=2.0) - (0.1 * 0.1058) = 1.9894\n",
    "print(w1) # w1_new = w1(=-3.0) - (0.1 * 0.2115) = -3.0212\n",
    "print(w2) # w2_new = w2(=-3.0) - (0.1 * 0.1058) = -2.9894\n",
    "\n",
    "q1 = w0*x0\n",
    "q2 = w1*x1\n",
    "\n",
    "r = q1 + q2\n",
    "s = r + w2\n",
    "\n",
    "out = torch.sigmoid(s)\n",
    "print(\"=====================\")\n",
    "print(\"Second Prediction\")\n",
    "# update된 weight를 바탕으로 두번째 prediction을 합니다.\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
